{"type": "text", "bbox": [21, 794, 802, 905], "res": [{"text": "We create the templates following what we believe end users would generally ask about documents (Table|1b. For KIE", "confidence": 0.9723905324935913, "text_region": [[24.0, 798.0], [801.0, 798.0], [801.0, 812.0], [24.0, 812.0]]}, {"text": "and CLS, we hypothesize that (1) the extraction instructions can teach DocLLM to correlate names of keys in the prompts.", "confidence": 0.9830166101455688, "text_region": [[21.0, 815.0], [803.0, 816.0], [803.0, 833.0], [21.0, 832.0]]}, {"text": "with document fields so as to retrieve values, (2) the internal classification instructions can help the model understand", "confidence": 0.9973459839820862, "text_region": [[24.0, 835.0], [803.0, 835.0], [803.0, 849.0], [24.0, 849.0]]}, {"text": "what intrinsically characterizes each key or document type, and (3) the multiple choice question (MCQ) instructions.", "confidence": 0.9929080605506897, "text_region": [[23.0, 852.0], [804.0, 852.0], [804.0, 870.0], [23.0, 870.0]]}, {"text": "can teach the model to leverage its comprehension of key names included as choices in the prompt (resp. document", "confidence": 0.9946696162223816, "text_region": [[24.0, 872.0], [803.0, 872.0], [803.0, 886.0], [24.0, 886.0]]}, {"text": "type names) to classify extracted values (resp. entire documents). We introduce the templates in detail as follows..", "confidence": 0.9882401823997498, "text_region": [[23.0, 890.0], [775.0, 890.0], [775.0, 904.0], [23.0, 904.0]]}], "img_idx": 0}
{"type": "text", "bbox": [22, 594, 803, 705], "res": [{"text": "Following recent work in the field of VRDU [12,[31, 32] and prior work in NLP [40] 41], we instruction-tune DocLLM on", "confidence": 0.9763750433921814, "text_region": [[22.0, 595.0], [803.0, 596.0], [803.0, 614.0], [22.0, 613.0]]}, {"text": "a variety of instructions derived from DocAI datasets using various templates. Due to the high cost and time intensity of", "confidence": 0.9890217781066895, "text_region": [[24.0, 617.0], [801.0, 617.0], [801.0, 631.0], [24.0, 631.0]]}, {"text": "manual data collection, we leave the construction of a VRDU instruction-tuning dataset with crowdsourced instructions", "confidence": 0.9907961487770081, "text_region": [[24.0, 636.0], [802.0, 636.0], [802.0, 650.0], [24.0, 650.0]]}, {"text": "and preferences to future work. We employ a total of 16 datasets with their corresponding OCRs, spanning four DocAI", "confidence": 0.9878364205360413, "text_region": [[24.0, 654.0], [802.0, 654.0], [802.0, 668.0], [24.0, 668.0]]}, {"text": "tasks: visual question answering (VQA), natural language inference (NLI), key information extraction (KIE), and", "confidence": 0.9868721961975098, "text_region": [[24.0, 672.0], [802.0, 672.0], [802.0, 686.0], [24.0, 686.0]]}, {"text": "document classification (CLS)", "confidence": 0.9972423911094666, "text_region": [[24.0, 689.0], [225.0, 689.0], [225.0, 703.0], [24.0, 703.0]]}], "img_idx": 0}
{"type": "text", "bbox": [22, 712, 804, 788], "res": [{"text": "The diversity of supervised fine tuning (SFT) instructions is critical in helping zero-shot generalization [40, 41, 2]", "confidence": 0.988934338092804, "text_region": [[22.0, 715.0], [803.0, 714.0], [803.0, 731.0], [22.0, 732.0]]}, {"text": "Thus, we diversify templates per task when possible, with each template asking a different question, and in some cases.", "confidence": 0.9874800443649292, "text_region": [[24.0, 736.0], [803.0, 736.0], [803.0, 750.0], [24.0, 750.0]]}, {"text": "expecting different types of answers. We re-use the templates introduced in [31] 32] when applicable, and consider a", "confidence": 0.9924561977386475, "text_region": [[22.0, 753.0], [804.0, 752.0], [804.0, 769.0], [22.0, 770.0]]}, {"text": "broader selection of datasets in our instruction-tuning data mix..", "confidence": 0.9952294230461121, "text_region": [[22.0, 771.0], [442.0, 772.0], [442.0, 786.0], [22.0, 785.0]]}], "img_idx": 0}
{"type": "text", "bbox": [22, 321, 804, 452], "res": [{"text": "contains a consecutive series of tokens. Further, let x be a corrupted version of x where the contiguous tokens", "confidence": 0.9938955903053284, "text_region": [[24.0, 345.0], [803.0, 345.0], [803.0, 359.0], [24.0, 359.0]]}, {"text": "corresponding to a sampled text block are replaced with a special mask token [M]. To facilitate the identification of", "confidence": 0.9927745461463928, "text_region": [[24.0, 363.0], [802.0, 363.0], [802.0, 377.0], [24.0, 377.0]]}, {"text": "the block to be filled during text generation, each input block is augmented with a special start token [S] while the.", "confidence": 0.9909090995788574, "text_region": [[23.0, 379.0], [801.0, 379.0], [801.0, 396.0], [23.0, 396.0]]}, {"text": "output block includes an end token [E]. For instance, a block with tokens (x4, x5) becomes [M] in x, ([S], x4, x5.", "confidence": 0.9790733456611633, "text_region": [[23.0, 398.0], [800.0, 398.0], [800.0, 415.0], [23.0, 415.0]]}, {"text": "when conditioned upon, and is expected to generate (x4, x5, E) as output autoregressively (see Figure|2[for a detailed", "confidence": 0.9754066467285156, "text_region": [[24.0, 418.0], [802.0, 418.0], [802.0, 432.0], [24.0, 432.0]]}, {"text": "illustration of these configurations). The following cross-entropy loss is then minimized for the infilling objective..", "confidence": 0.992837131023407, "text_region": [[23.0, 434.0], [779.0, 434.0], [779.0, 451.0], [23.0, 451.0]]}], "img_idx": 0}
{"type": "title", "bbox": [22, 561, 195, 581], "res": [{"text": "3.4Instruction Tuning", "confidence": 0.9992972612380981, "text_region": [[22.0, 563.0], [193.0, 563.0], [193.0, 580.0], [22.0, 580.0]]}], "img_idx": 0}
{"type": "table", "bbox": [21, 91, 802, 287], "res": {"cell_bbox": [[2.7167952060699463, 5.544087886810303, 25.79660415649414, 18.939167022705078], [40.80739974975586, 4.982592582702637, 112.37027740478516, 19.755271911621094], [166.3546142578125, 5.313100814819336, 268.3370361328125, 19.475208282470703], [618.2561645507812, 5.294015884399414, 729.913330078125, 19.746522903442383], [2.1075851917266846, 27.757633209228516, 36.81938934326172, 42.22798538208008], [36.87348556518555, 27.325294494628906, 83.22650146484375, 42.52015686035156], [173.79953002929688, 27.068056106567383, 327.28009033203125, 42.68331527709961], [627.2791137695312, 27.93891716003418, 731.494140625, 42.81261444091797], [2.337785005569458, 48.353546142578125, 32.393699645996094, 64.14617919921875], [41.72827911376953, 47.15000915527344, 84.15431213378906, 63.55392074584961], [193.27163696289062, 46.54466247558594, 416.9626159667969, 63.540191650390625], [619.4749145507812, 47.37187576293945, 727.4821166992188, 62.23322296142578], [2.4367001056671143, 77.1083984375, 22.93353271484375, 92.02954864501953], [43.63227462768555, 69.65554809570312, 82.54973602294922, 87.16018676757812], [179.3388671875, 68.87651062011719, 438.1418762207031, 86.5350570678711], [619.0487670898438, 71.22684478759766, 771.8720703125, 85.04387664794922], [45.224700927734375, 93.3888168334961, 80.68875885009766, 111.43335723876953], [168.7586212158203, 93.19384765625, 604.0711669921875, 116.4316635131836], [619.1944580078125, 93.2685317993164, 762.993408203125, 107.55630493164062], [26.066898345947266, 122.77473449707031, 137.2925262451172, 141.06243896484375], [164.86341857910156, 121.4660415649414, 442.6341552734375, 141.38082885742188], [625.7490234375, 122.3945541381836, 756.01513671875, 137.5920867919922], [2.436412811279297, 159.4088134765625, 21.33401107788086, 174.30364990234375], [54.852149963378906, 149.9689483642578, 87.61634063720703, 166.95950317382812], [182.0843963623047, 147.53953552246094, 564.9346313476562, 170.22425842285156], [634.3280029296875, 148.03038024902344, 722.3084716796875, 163.8848419189453], [40.206363677978516, 174.803466796875, 133.57135009765625, 190.66282653808594], [168.8402862548828, 175.36598205566406, 405.60272216796875, 191.5029754638672], [619.7686767578125, 176.01075744628906, 715.17529296875, 191.01715087890625]], "html": "<html><body><table><thead><tr><td>Task</td><td>Template type</td><td> Prompt template</td><td>Expected response</td></tr></thead><tbody><tr><td>VQA</td><td>Extraction</td><td>\"document} {quest ion}\"</td><td> answer annotation</td></tr><tr><td>NLI</td><td>MCQ</td><td>\" document } V{statement }\", Yes or No?\"</td><td> answer annotation</td></tr><tr><td rowspan=\"3\">KIE</td><td>Extraction</td><td>\"{ document } What is the value for the I\"{key}\"?\"</td><td> Associated value annotation</td></tr><tr><td>MCQ</td><td>\"{document } What is I\"{value}\\\" in the document? Possible choices: { choices}.\" (where choices is a subset of allthe keys in the dataset in random order).</td><td>Associated key annotation</td></tr><tr><td> Internal classification</td><td>\"{document } What is I\"{value}\" in the document?\"?</td><td>Associated key annotation</td></tr><tr><td rowspan=\"2\">CLS</td><td>MCQ</td><td>\"{document } What type of document is this? Possible choices: {choices}.\" (where choices is a subset of all the classes in the dataset in random order).</td><td> class annotation</td></tr><tr><td>Internal classification</td><td>\"document } What type of document is this?\"</td><td> class annotation</td></tr></tbody></table></body></html>"}, "img_idx": 0}
